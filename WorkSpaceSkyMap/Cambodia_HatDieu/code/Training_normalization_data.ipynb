{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr  7 16:15:55 2022\n",
    "\n",
    "@author: SkyMap\n",
    "\"\"\"\n",
    "\n",
    "# from email.mime import base\n",
    "# from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import geopandas as gp\n",
    "import os, glob\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, ReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from keras.models import load_model\n",
    "    \n",
    "import rasterio\n",
    "from sklearn import datasets\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lấy thông tin của label và nodata, chuyển hết dữ liệu về dạng flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_and_mask_train(fp_mask, nodata_value=100):\n",
    "    src = rasterio.open(fp_mask)\n",
    "    mask = src.read()[0].flatten()\n",
    "    index_nodata = np.where(mask == nodata_value)\n",
    "    mask_train = np.delete(mask, index_nodata)\n",
    "    return mask_train, index_nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuyển dữ liệu về dạng dataframe, trong đó xóa luôn dữ liệu nodata để giảm kích thước DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_flatten_train(fp_img, list_number_band, index_nodata):\n",
    "    src = rasterio.open(fp_img)\n",
    "    # return to img train\n",
    "    list_band_have = list(range(1,src.count+1))\n",
    "    dfObj = pd.DataFrame()\n",
    "    if set(list_number_band).issubset(list_band_have):\n",
    "        img = src.read(list_number_band)\n",
    "        i = 0\n",
    "        for band in img:\n",
    "            band = band.flatten()\n",
    "            band = np.delete(band, index_nodata)\n",
    "            name_band = f\"band {list_number_band[i]}\"\n",
    "            dfObj[name_band] = band\n",
    "            i+=1\n",
    "        return dfObj\n",
    "    else:\n",
    "        miss = np.setdiff1d(list_number_band, list_band_have)\n",
    "        print(\"*\"*15, \"ERROR\", \"*\"*15)\n",
    "        print(f\"Image dont have band : {miss.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_train_from_ones_img(fp_img, fp_mask, list_band_to_train, size_get=3000):\n",
    "    mask_train, index_nodata = get_index_and_mask_train(fp_mask)\n",
    "    df_dataset = get_df_flatten_train(fp_img, list_band_to_train, index_nodata)\n",
    "    df_dataset['label'] = mask_train\n",
    "\n",
    "\n",
    "    g = df_dataset.groupby('label', group_keys=False)\n",
    "    g = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "    g = pd.DataFrame(g)\n",
    "    # size_get = 3000       # sample size\n",
    "    replace = False  # with replacement\n",
    "    fn = lambda obj: obj.loc[np.random.choice(obj.index, size_get, replace),:]\n",
    "    a= g.groupby('label', as_index=False).apply(fn)\n",
    "    # print(a)\n",
    "    return pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_train_all_img(list_fp_img, list_fp_mask, list_band_to_train, out_fp_csv_train):\n",
    "    print(list_fp_img)\n",
    "    dir_name_img = os.path.dirname(list_fp_img[0])\n",
    "    list_df_all = []\n",
    "    for fp_mask in list_fp_mask:\n",
    "        base_name = os.path.basename(fp_mask)\n",
    "        fp_img = os.path.join(dir_name_img, base_name)\n",
    "        df_tmp = create_data_train_from_ones_img(fp_img, fp_mask, list_band_to_train)\n",
    "        # print(df_tmp)\n",
    "        list_df_all.append(df_tmp)\n",
    "        result = pd.concat(list_df_all)\n",
    "        # print(result)\n",
    "    # print(np.unique(result['label'].to_numpy()))\n",
    "    result.to_csv(out_fp_csv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_train(csv_training):\n",
    "    datasets = pd.read_csv(csv_training).iloc[:, 2:]\n",
    "    print(datasets.shape)\n",
    "    X = datasets.iloc[:, :-1]\n",
    "    Y = datasets.iloc[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    Y = np_utils.to_categorical(encoded_Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_fp_csv_train = r'E:\\WORK\\Cambodia_HatDieu\\Data\\training_fix_012.csv'\n",
    "# datasets = pd.read_csv(out_fp_csv_train).iloc[:, 2:]\n",
    "# X = datasets.iloc[:, :-1]\n",
    "# Y = datasets.iloc[:, -1]\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(Y)\n",
    "# encoded_Y = encoder.transform(Y)\n",
    "# Y = np_utils.to_categorical(encoded_Y)\n",
    "# Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mạng DENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, label, classes=7, epochs=100, batch_size=100, shuffle=True, model_path='model.h5'):\n",
    "    assert classes>=2, 'number classese must be more than 1'\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = 4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    # model.add(Dense(10, activation = 'relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(ReLU())\n",
    "\n",
    "    # model.add(Dense(10, activation = 'relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(ReLU())\n",
    "    if classes==2:\n",
    "        model.add(Dense(2, activation = 'sigmoid'))\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        model.add(Dense(classes, activation = 'softmax'))\n",
    "        loss='categorical_crossentropy'\n",
    "\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(input, label, epochs=epochs, batch_size=batch_size, shuffle=shuffle)\n",
    "    scores = model.evaluate(input, label)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    list_fp_img = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\img\\*.tif')\n",
    "    list_fp_mask = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\mask_fix_012\\*.tif')\n",
    "    list_band_to_train = [1,2,3,4]\n",
    "    out_fp_csv_train = r'E:\\WORK\\Cambodia_HatDieu\\Data\\training_fix_012_3000.csv'\n",
    "    model_path=r\"E:\\WORK\\Cambodia_HatDieu\\Data\\model_3000.h5\"\n",
    "\n",
    "    if not os.path.exists(out_fp_csv_train):\n",
    "        create_data_train_all_img(list_fp_img, list_fp_mask, list_band_to_train, out_fp_csv_train)\n",
    "    X, Y = create_data_train(out_fp_csv_train)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print('Traing...')\n",
    "    train(X, Y, classes=3, epochs=1000, batch_size=100000, shuffle=True, model_path=model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training mạng Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 5)\n",
      "(75000, 4)\n",
      "(75000, 3)\n",
      "Traing...\n",
      "WARNING:tensorflow:From C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "75000/75000 [==============================] - 2s 32us/step - loss: 1.0960 - accuracy: 0.5669\n",
      "Epoch 2/1000\n",
      "75000/75000 [==============================] - 0s 1us/step - loss: 1.0759 - accuracy: 0.5762\n",
      "Epoch 3/1000\n",
      "75000/75000 [==============================] - 0s 1us/step - loss: 1.0600 - accuracy: 0.6782\n",
      "Epoch 4/1000\n",
      "75000/75000 [==============================] - 0s 1us/step - loss: 1.0446 - accuracy: 0.6832\n",
      "Epoch 5/1000\n",
      "75000/75000 [==============================] - 0s 1us/step - loss: 1.0299 - accuracy: 0.6877\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mạng XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data training with 80% train and 20%test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_data_train_Xgboost(csv_training, training_per = 0.8):\n",
    "    datasets = pd.read_csv(csv_training).iloc[:, 2:]\n",
    "    ds_train = datasets.sample(frac=training_per)\n",
    "    ds_test = datasets[~datasets.isin(ds_train)].dropna()\n",
    "\n",
    "    X_train = ds_train.iloc[:, :-1]\n",
    "    Y_train = ds_train.iloc[:, -1]\n",
    "    X_test = ds_test.iloc[:, :-1]\n",
    "    Y_test = ds_test.iloc[:, -1]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle\n",
    "\n",
    "def XGboost_train():\n",
    "    list_fp_img = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\img\\*.tif')\n",
    "    list_fp_mask = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\mask_fix_012\\*.tif')\n",
    "    list_band_to_train = [1,2,3,4]\n",
    "    out_fp_csv_train = r'E:\\WORK\\Cambodia_HatDieu\\Data\\training_fix_012_3000.csv'\n",
    "    fp_model_save = r\"E:\\WORK\\Cambodia_HatDieu\\Data\\model_3000_v2.pkl\"\n",
    "    if not os.path.exists(out_fp_csv_train):\n",
    "        create_data_train_all_img(list_fp_img, list_fp_mask, list_band_to_train, out_fp_csv_train)\n",
    "    X_train, Y_train, X_test, Y_test = create_data_train_Xgboost(out_fp_csv_train, training_per = 0.8)\n",
    "    print('Training ...')\n",
    "    clf = GradientBoostingClassifier(n_estimators=10000, learning_rate=1.0, max_depth=3, random_state=0).fit(X=X_train, y=Y_train)\n",
    "    print('acc: ',clf.score(X_test, Y_test))\n",
    "    with open(fp_model_save, 'wb') as file:\n",
    "        pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.8974\n"
     ]
    }
   ],
   "source": [
    "XGboost_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(r\"E:\\WORK\\Cambodia_HatDieu\\agaricus.txt.test\")\n",
    "dtest = xgb.DMatrix(r\"E:\\WORK\\Cambodia_HatDieu\\agaricus.txt.train.txt\")\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c\n",
       "0  0   1   2\n",
       "1  3   4   5\n",
       "2  6   7   8\n",
       "3  9  10  11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])\n",
    "label = pd.DataFrame(np.random.randint(2, size=4))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST WITH LIB FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import glob, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_train_Xgboost(csv_training, training_per = 0.8):\n",
    "    datasets = pd.read_csv(csv_training).iloc[:, 2:]\n",
    "    ds_train = datasets.sample(frac=training_per)\n",
    "    ds_test = datasets[~datasets.isin(ds_train)].dropna()\n",
    "\n",
    "    X_train = ds_train.iloc[:, :-1]\n",
    "    Y_train = ds_train.iloc[:, -1]\n",
    "    X_test = ds_test.iloc[:, :-1]\n",
    "    Y_test = ds_test.iloc[:, -1]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "[0]\teval-auc:0.92782\ttrain-auc:0.92583\n",
      "[1]\teval-auc:0.94900\ttrain-auc:0.94712\n",
      "[2]\teval-auc:0.95662\ttrain-auc:0.95468\n",
      "[3]\teval-auc:0.95988\ttrain-auc:0.95865\n",
      "[4]\teval-auc:0.96275\ttrain-auc:0.96164\n",
      "[5]\teval-auc:0.96355\ttrain-auc:0.96246\n",
      "[6]\teval-auc:0.96464\ttrain-auc:0.96350\n",
      "[7]\teval-auc:0.96490\ttrain-auc:0.96392\n",
      "[8]\teval-auc:0.96593\ttrain-auc:0.96532\n",
      "[9]\teval-auc:0.96777\ttrain-auc:0.96745\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_fp_img = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\img\\*.tif')\n",
    "list_fp_mask = glob.glob(r'E:\\WORK\\Cambodia_HatDieu\\Data\\mask_fix_012\\*.tif')\n",
    "list_band_to_train = [1,2,3,4]\n",
    "out_fp_csv_train = r'E:\\WORK\\Cambodia_HatDieu\\Data\\training_fix_012_9000.csv'\n",
    "fp_model_save = r\"E:\\WORK\\Cambodia_HatDieu\\Data\\model_9000_v2.model\"\n",
    "if not os.path.exists(out_fp_csv_train):\n",
    "    create_data_train_all_img(list_fp_img, list_fp_mask, list_band_to_train, out_fp_csv_train)\n",
    "X_train, Y_train, X_test, Y_test = create_data_train_Xgboost(out_fp_csv_train, training_per = 0.8)\n",
    "print('Training ...')\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=Y_test)\n",
    "num_round = 10\n",
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'multi:softmax'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "param['num_class'] = 3\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "bst.save_model(fp_model_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc63a6eef95843afe201fce49e80730796a68a524bff5092aa076c4c583efa68"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
