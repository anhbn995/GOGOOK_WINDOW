{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc0eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate as merge_l\n",
    "\n",
    "from keras.optimizers import Adam, Nadam, Adadelta,SGD\n",
    "from keras.layers import (\n",
    "    Input, Convolution2D, MaxPooling2D, UpSampling2D,\n",
    "    Reshape, core, Dropout, Flatten,\n",
    "    Activation, BatchNormalization, Lambda, Dense, Conv2D, Conv2DTranspose, concatenate,Permute,Cropping2D,Add)\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import (ModelCheckpoint, TensorBoard, CSVLogger, History, EarlyStopping, LambdaCallback,ReduceLROnPlateau)\n",
    "from data_defores_generator import data_gen, create_list_id, split_black_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305bbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    smooth = 1e-12\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    smooth = 1e-12\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0febe5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(num_channel,size):\n",
    "    conv_params = dict(activation='relu', border_mode='same')\n",
    "    merge_params = dict(axis=-1)\n",
    "    inputs1 = Input((size, size,int(num_channel)))\n",
    "    conv1 = Convolution2D(32, (3,3), **conv_params)(inputs1)\n",
    "    conv1 = Convolution2D(32, (3,3), **conv_params)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, (3,3), **conv_params)(pool1)\n",
    "    conv2 = Convolution2D(64, (3,3), **conv_params)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, (3,3), **conv_params)(pool2)\n",
    "    conv3 = Convolution2D(128, (3,3), **conv_params)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, (3,3), **conv_params)(pool3)\n",
    "    conv4 = Convolution2D(256, (3,3), **conv_params)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, (3,3), **conv_params)(pool4)\n",
    "    conv5 = Convolution2D(512, (3,3), **conv_params)(conv5)\n",
    "\n",
    "    up6 = merge_l([UpSampling2D(size=(2, 2))(conv5), conv4], **merge_params)\n",
    "    conv6 = Convolution2D(256, (3,3), **conv_params)(up6)\n",
    "    conv6 = Convolution2D(256, (3,3), **conv_params)(conv6)\n",
    "\n",
    "    up7 = merge_l([UpSampling2D(size=(2, 2))(conv6), conv3], **merge_params)\n",
    "    conv7 = Convolution2D(128, (3,3), **conv_params)(up7)\n",
    "    conv7 = Convolution2D(128, (3,3), **conv_params)(conv7)\n",
    "\n",
    "    up8 = merge_l([UpSampling2D(size=(2, 2))(conv7), conv2], **merge_params)\n",
    "    conv8 = Convolution2D(64, (3,3), **conv_params)(up8)\n",
    "    conv8 = Convolution2D(64, (3,3), **conv_params)(conv8)\n",
    "\n",
    "    up9 = merge_l([UpSampling2D(size=(2, 2))(conv8), conv1], **merge_params)\n",
    "    conv9 = Convolution2D(32, (3,3), **conv_params)(up9)\n",
    "    conv9 = Convolution2D(32, (3,3), **conv_params)(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "    optimizer=SGD(lr=1e-4, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "    model = Model(input=inputs1, output=conv10)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=binary_crossentropy,\n",
    "                metrics=['accuracy', jaccard_coef, jaccard_coef_int])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beb4bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SkyMap\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 32) 1184        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 32) 9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 256, 256, 32) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 128 147584      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 768)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  1769728     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 384 0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 128 442496      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 128, 128 147584      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 192 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 256, 64) 110656      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 512, 512, 64) 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512, 512, 96) 0           up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 512, 512, 32) 27680       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 512, 512, 32) 9248        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 512, 512, 1)  33          conv2d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,846,945\n",
      "Trainable params: 7,846,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d020e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SkyMap\\\\Desktop\\\\Test\\\\Test\\\\KGX_Unet_512_Nuoc\\\\Ngay07Thang09Nam2021_14h_05p_14s'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "CURRENT_DATE = datetime.now().strftime(\"Ngay%dThang%mNam%Y_%Hh_%Mp_%Ss\")\n",
    "BATCH_SIZE=2\n",
    "NUM_CHANNEL=4\n",
    "NUM_CLASS=1\n",
    "INPUT_SIZE=512\n",
    "NUM_EPOCH = 500\n",
    "EARLY=70\n",
    "SPLIT_RATIO=0.9\n",
    "MODEL_NAME=\"KGX_Unet_512_Nuoc\"\n",
    "IMAGE_PATH=r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\KGX_Unet_512_Nuoc\\TrainingDataset\\img_crop\"\n",
    "MASK_PATH=r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\KGX_Unet_512_Nuoc\\TrainingDataset\\img_mask_crop\"\n",
    "MODEL_DIR=r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\{}\\{}\".format(MODEL_NAME, CURRENT_DATE)\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef79c4b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0981148530a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munet_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_CHANNEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m model_checkpoint = ModelCheckpoint(\n\u001b[0;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"{}_val_weights.h5\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCURRENT_DATE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_{epoch:04d}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unet' is not defined"
     ]
    }
   ],
   "source": [
    "unet_model = unet(NUM_CHANNEL, INPUT_SIZE)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, \"{}_val_weights.h5\".format(CURRENT_DATE + \"_{epoch:04d}\")),\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False)\n",
    "model_earlystop = EarlyStopping(\n",
    "        patience=EARLY,\n",
    "        verbose=0,\n",
    "        )\n",
    "model_history = History()\n",
    "model_board = TensorBoard(\n",
    "    log_dir=os.path.join(MODEL_DIR, 'logs'),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    embeddings_freq=0)\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    factor=np.sqrt(0.1),\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    min_lr=0.5e-7)\n",
    "image_list= create_list_id(IMAGE_PATH)\n",
    "np.random.shuffle(image_list)\n",
    "count = len(image_list)    \n",
    "cut_idx = int(round(count*SPLIT_RATIO))    \n",
    "train_list = image_list[0:cut_idx]\n",
    "val_list = [id_image for id_image in image_list if id_image not in train_list]\n",
    "\n",
    "thres_neg = 1/100.0\n",
    "scale_neg = 20/100.0\n",
    "print(\"Train samples: {}\".format(len(train_list)))\n",
    "print(\"Validation samples: {}\".format(len(val_list)))   \n",
    "print(\"===========================\")\n",
    "\n",
    "pos_train,neg_train = split_black_image(image_list,MASK_PATH,thres_neg)\n",
    "pos_val,neg_val = split_black_image(image_list,MASK_PATH,thres_neg)\n",
    "step1 = round(len(pos_train)*(1+scale_neg)/BATCH_SIZE)\n",
    "step2 = round(len(pos_val)*(1+scale_neg)/BATCH_SIZE)\n",
    "num_chanel=NUM_CHANNEL\n",
    "print(step2)\n",
    "unet_model.fit_generator(\n",
    "    generator= data_gen(pos_train,neg_train, IMAGE_PATH, MASK_PATH, BATCH_SIZE, scale_neg,num_chanel,NUM_CLASS,augment = True),\n",
    "    validation_data = data_gen(pos_val,neg_val, IMAGE_PATH, MASK_PATH, BATCH_SIZE, scale_neg,num_chanel,NUM_CLASS),\n",
    "    validation_steps = 100,\n",
    "    steps_per_epoch = 500, \n",
    "    epochs=NUM_EPOCH, \n",
    "    verbose=1, \n",
    "    callbacks=[model_checkpoint,model_earlystop, model_history, model_board,lr_reducer])\n",
    "unet_model.save_weights(os.path.join(MODEL_DIR, \"{}_val_weights_last.h5\".format(CURRENT_DATE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91b73978",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MOEL = 512\n",
    "input_size = 512\n",
    "crop_size = 400\n",
    "num_chanel = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fe33e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_window_many_chanel(output_ds, arr_c, s_h, e_h ,s_w, e_w, sw_w, sw_h, size_w_crop, size_h_crop):\n",
    "    for c, arr in enumerate(arr_c):\n",
    "        output_ds.write(arr[s_h:e_h,s_w:e_w],window = Window(sw_w, sw_h, size_w_crop, size_h_crop), indexes= c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39175e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_small(cnn_model, img, dict_min_max=None):\n",
    "    if dict_min_max:\n",
    "        img = create_img_01(img, dict_min_max)\n",
    "    img = img.swapaxes(0,1).swapaxes(1,2)\n",
    "    img = cv2.resize(img,(INPUT_MOEL, INPUT_MOEL), interpolation = cv2.INTER_CUBIC)\n",
    "    y_pre = cnn_model.predict(np.array([img/255]))\n",
    "    y_pre = y_pre.reshape((INPUT_MOEL,INPUT_MOEL))\n",
    "    y_pre =(y_pre>0.5).astype(np.uint8)\n",
    "    y_pre = cv2.resize(y_pre,(input_size, input_size), interpolation = cv2.INTER_CUBIC)\n",
    "    y_pre = np.array([y_pre])*255\n",
    "    return y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707a816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59606081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_big(image_path, outputFileName1, dict_model):\n",
    "    \"\"\"\n",
    "        Ham nay dung de predict 1 anh lon theo tung cua so truot.\n",
    "        INPUT:\n",
    "            image_path     : Duong dan anh can nhan dien\n",
    "            outputFileName1: Duong dan file ket qua\n",
    "            dict_model     : Chua ten class va duong dan model cua class.\n",
    "    \"\"\"\n",
    "    head, tail  = os.path.split(outputFileName1)\n",
    "    for key in dict_model:\n",
    "        out_dir_name = os.path.join(head,key)\n",
    "        if not os.path.exists(out_dir_name):\n",
    "            os.makedirs(out_dir_name)\n",
    "        outputFileName = os.path.join(out_dir_name, tail)\n",
    "        fp_model = dict_model[key]\n",
    "\n",
    "        # outputFileName = outputFileName[:-4] + f\"_{key}.tif\"\n",
    "        cnn_model = unet_model(num_channel=num_chanel, size=INPUT_MOEL)\n",
    "        cnn_model.load_weights(fp_model)\n",
    "\n",
    "        # dict_min_max = get_min_max_image(image_path)\n",
    "        dict_min_max = None\n",
    "        with rasterio.open(image_path) as src:\n",
    "            h,w = src.height,src.width\n",
    "            source_crs = src.crs\n",
    "            source_transform = src.transform\n",
    "            # dtype_or = src.dtypes\n",
    "            num_band = src.count\n",
    "\n",
    "        # create img predict one band \n",
    "        with rasterio.open(outputFileName, 'w', driver='GTiff',\n",
    "                                    height = h, width = w,\n",
    "                                    count=1, dtype='uint8',\n",
    "                                    crs=source_crs,\n",
    "                                    transform=source_transform,\n",
    "                                    nodata=0,\n",
    "                                    compress='lzw') as output_ds:\n",
    "            output_ds = np.empty((num_band,h,w))\n",
    "            \n",
    "        padding = int((input_size - crop_size)/2)\n",
    "        list_weight = list(range(0, w, crop_size))\n",
    "        list_hight = list(range(0, h, crop_size))\n",
    "\n",
    "        src = rasterio.open(image_path)\n",
    "        with rasterio.open(outputFileName,\"r+\") as output_ds:\n",
    "            with tqdm(total=len(list_hight)*len(list_weight)) as pbar:\n",
    "                for start_h_org in list_hight:\n",
    "                    for start_w_org in list_weight:\n",
    "                        # vi tri bat dau\n",
    "                        h_crop_start = start_h_org - padding\n",
    "                        w_crop_start = start_w_org - padding\n",
    "                        # kich thuoc\n",
    "                        tmp_img_size_model = np.zeros((num_band, input_size,input_size))\n",
    "                        # truong hop 0 0\n",
    "                        if h_crop_start < 0 and w_crop_start < 0:\n",
    "                            # continue\n",
    "                            h_crop_start = 0\n",
    "                            w_crop_start = 0\n",
    "                            size_h_crop = crop_size + padding\n",
    "                            size_w_crop = crop_size + padding\n",
    "                            img_window_crop  = src.read(window=Window(w_crop_start, h_crop_start, size_w_crop, size_h_crop))\n",
    "                            tmp_img_size_model[:, padding:, padding:] = img_window_crop\n",
    "                            img_predict = predict_small(cnn_model, tmp_img_size_model, dict_min_max)\n",
    "                            write_window_many_chanel(output_ds, img_predict, padding, crop_size + padding, padding, \n",
    "                                                     crop_size + padding, start_w_org, start_h_org, crop_size, crop_size)\n",
    "                        \n",
    "                        # truong hop h = 0 va w != 0\n",
    "                        elif h_crop_start < 0:\n",
    "                            h_crop_start = 0\n",
    "                            size_h_crop = crop_size + padding\n",
    "                            size_w_crop = min(crop_size + 2*padding, w - start_w_org + padding)\n",
    "                            img_window_crop  = src.read(window=Window(w_crop_start, h_crop_start, size_w_crop, size_h_crop))\n",
    "                            \n",
    "                            if size_w_crop == w - start_w_org + padding:\n",
    "                                end_c_index_w =  size_w_crop\n",
    "                                tmp_img_size_model[:,padding:,:end_c_index_w] = img_window_crop\n",
    "                            else:\n",
    "                                end_c_index_w = crop_size + padding\n",
    "                                tmp_img_size_model[:, padding:,:] = img_window_crop\n",
    "                            img_predict = predict_small(cnn_model, tmp_img_size_model, dict_min_max)\n",
    "                            write_window_many_chanel(output_ds, img_predict, padding, crop_size + padding ,padding, \n",
    "                                                     end_c_index_w, start_w_org, start_h_org,  \n",
    "                                                     min(crop_size, w - start_w_org), crop_size)\n",
    "                        \n",
    "                        # Truong hop w = 0, h!=0 \n",
    "                        elif w_crop_start < 0:\n",
    "                            w_crop_start = 0\n",
    "                            size_w_crop = crop_size + padding\n",
    "                            size_h_crop = min(crop_size + 2*padding, h - start_h_org + padding)\n",
    "                            img_window_crop  = src.read(window=Window(w_crop_start, h_crop_start, size_w_crop, size_h_crop))\n",
    "                            \n",
    "                            if size_h_crop == h - start_h_org + padding:\n",
    "                                end_c_index_h =  size_h_crop\n",
    "                                tmp_img_size_model[:,:end_c_index_h,padding:] = img_window_crop\n",
    "                            else:\n",
    "                                end_c_index_h = crop_size + padding\n",
    "                                tmp_img_size_model[:,:, padding:] = img_window_crop\n",
    "                            img_predict = predict_small(cnn_model, tmp_img_size_model, dict_min_max)\n",
    "                            write_window_many_chanel(output_ds, img_predict, padding, end_c_index_h, padding,crop_size + padding\n",
    "                                                     , start_w_org, start_h_org, crop_size, min(crop_size, h - start_h_org))\n",
    "                            \n",
    "                        # Truong hop ca 2 deu khac khong\n",
    "                        else:\n",
    "                            size_w_crop = min(crop_size +2*padding, w - start_w_org + padding)\n",
    "                            size_h_crop = min(crop_size +2*padding, h - start_h_org + padding)\n",
    "                            img_window_crop  = src.read(window=Window(w_crop_start, h_crop_start, size_w_crop, size_h_crop))\n",
    "                            # print(img_window_crop.shape, size_w_crop, size_h_crop)\n",
    "                            if size_w_crop < (crop_size + 2*padding) and size_h_crop < (crop_size + 2*padding):\n",
    "                                # print(img_window_crop.shape, size_w_crop, size_h_crop)\n",
    "                                end_c_index_h = size_h_crop\n",
    "                                end_c_index_w = size_w_crop\n",
    "                                tmp_img_size_model[:,:end_c_index_h,:   end_c_index_w] = img_window_crop\n",
    "                            elif size_w_crop < (crop_size + 2*padding):\n",
    "                                end_c_index_h = crop_size + padding\n",
    "                                end_c_index_w = size_w_crop\n",
    "                                tmp_img_size_model[:,:,:end_c_index_w] = img_window_crop\n",
    "                            elif size_h_crop < (crop_size + 2*padding):\n",
    "                                end_c_index_w = crop_size + padding\n",
    "                                end_c_index_h = size_h_crop\n",
    "                                tmp_img_size_model[:,:end_c_index_h,:] = img_window_crop\n",
    "                            else:\n",
    "                                end_c_index_w = crop_size + padding\n",
    "                                end_c_index_h = crop_size + padding\n",
    "                                tmp_img_size_model[:,:,:] = img_window_crop\n",
    "                            img_predict = predict_small(cnn_model, tmp_img_size_model, dict_min_max) \n",
    "                            write_window_many_chanel(output_ds, img_predict, padding, end_c_index_h, padding, end_c_index_w, \n",
    "                                                     start_w_org, start_h_org, \n",
    "                                                     min(crop_size, w - start_w_org), min(crop_size, h - start_h_org))\n",
    "                        pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b5e9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    " dict_fp_model = {\n",
    "        \"Nuoc\":r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\KGX_Unet_512_Nuoc\\Ngay07Thang09Nam2021_13h_14p_11.h5\",\n",
    "        \"CoBui\":r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\KGX_Unet_512_CoBui\\Ngay08Thang09Nam2021_1h_07p_02.h5\",\n",
    "        \"CoTan\":r\"C:\\Users\\SkyMap\\Desktop\\Test\\Test\\KGX_Unet_512_CoTan\\Ngay09Thang09Nam2021_5h_45p_33.h5\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892138e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
